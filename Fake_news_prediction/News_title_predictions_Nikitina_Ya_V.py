#!/usr/bin/env python
# coding: utf-8

# <div class="alert alert-block alert-info">
#  
# <h1><center>–ü–†–û–ï–ö–¢: </center></h1>
# <h1><center>"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ –∏ –ª–æ–∂–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"</center></h1>
#     
# </div>

# <div class="alert alert-block alert-success">
# 
# **–í –Ω–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ –∏–º–µ–µ–µ—Ç—Å—è –¥–≤–∞ —Ñ–∞–π–ª–∞ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏.**
#     
# **–§–∞–π–ª "train.tsv" —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Å–æ—Å—Ç–æ—è—à–∏–µ –∏–∑ –¥–≤—É—Ö –∫–æ–ª–æ–Ω–æ–∫: *`'title'`* (–∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π) –∏ *`'is_fake'`* (–∫–ª–∞—Å—Å 0 - —Ä–µ–∞–ª—å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å, –∫–ª–∞—Å—Å 1 - –≤—ã–¥—É–º–∞–Ω–Ω–∞—è).** 
#     
# **–í —Ñ–∞–π–ª–µ *`test.tsv`* —Ç–∞–∫–∂–µ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π, —Ü–µ–ª–µ–≤—É—é –∂–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å.**
# 
#     
# <h1><span style="text-decoration:underline">–¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞ - –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç—å vs –ª–æ–∂–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞–º.</span></h1>
#     
# </div>

# **–ü–õ–ê–ù –†–ê–ë–û–¢–´:**
# 
# **1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.**
# 
# **2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫ –æ–±—É—á–µ–Ω–∏—é.**
# 
# **3. –û–±—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –¥–ª—è –µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ.**
# 
# **4. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö/–≤—ã–¥—É–º–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ.**
# 
# <h1><center>–í –ø—É—Ç—å!</center></h1>

# <img src="https://cdn-images-1.medium.com/max/1600/1%2AlG256YbUI4Cptj8SO1ry0A.gif" width="500">

# # Installing wordcloud package

# In[1]:


pip install wordcloud


# ##  Importing libraries

# In[2]:


import pandas as pd
import numpy as np

from sklearn import *

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix, roc_curve, roc_auc_score

from wordcloud import WordCloud
from pymystem3 import Mystem
import re, nltk
from nltk.corpus import stopwords

import  matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')

import warnings
warnings.simplefilter(action='ignore', category=UserWarning)
warnings.simplefilter(action='ignore', category=FutureWarning)


# ## Importing and reading training data

# In[3]:


data = pd.read_csv('train.tsv', sep='\t')


# In[4]:


data.head(20)


# In[5]:


data.info()


# –ò–º–µ–µ–º 5758 —Å—Ç—Ä–æ–∫. –ù–∞ –ø–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –¥–∞–Ω–Ω—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –¥–æ–≤–æ–ª—å–Ω–æ "—á–∏—Å—Ç—ã–º–∏": –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –≥—Ä—É–±—ã—Ö –ø–µ—Ä–µ–±–æ–µ–≤ –≤ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ö, –Ω–∞–ª–∏—á–∏–∏ –Ω–µ–Ω—É–∂–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä. –ü—Ä–æ–ø—É—Å–∫–æ–≤ —Ç–∞–∫–∂–µ –Ω–µ—Ç ‚úîÔ∏è. 
# 
# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:

# In[6]:


# Fake amd non-fake news ratio

ratio = data['is_fake'].value_counts()
print(ratio, '\n')

plt.figure(figsize=(5,5))
plt.title('–î–æ–ª—è –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∏ –ª–æ–∂–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π', fontsize=14, color='black')

labels=['NON-FAKE', 'FAKE']
ratio.plot(kind='pie', startangle=90, textprops={'fontsize': 14},
           labels=labels, autopct = '%0.0f%%', colors = ['green', 'red'])
plt.ylabel('');


# –ê–±—Å–æ–ª—é—Ç–Ω–æ–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ - –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –æ—Ç—Å—É—Ç–≤—É–µ—Ç ‚úîÔ∏è.
# 
# –ü—Ä–æ–≤–µ—Ä–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã:

# In[7]:


data['title'].duplicated().sum()


# –û–¥–∏–Ω –ø–æ–ø–∞–ª—Å—è. –ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–∫—Ä–∞–ª—Å—è –≤ –¥–∞–Ω–Ω—ã–µ –¥–≤–∞–∂–¥—ã:

# In[8]:


data['title'].value_counts()


# –ú–º–º..–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ)) —É–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç ‚úîÔ∏è

# In[9]:


data = data.drop_duplicates(subset=['title']) 


# –ü–æ—Å–º–æ—Ç—Ä–∏–º –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–µ–µ –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π

# In[10]:


# Displaying full length title strings on a screen

def display_max_str_width(data, width):
    with pd.option_context('display.max_colwidth', width):
        print(data)

display_max_str_width(data['title'], 100)


# –¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∞–ª—Ñ–∞–≤–∏—Ç, —Ü–∏—Ñ—Ä—ã –∏ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏. –ü—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Ç–µ–∫—Å—Ç–∞ –º—ã —Å–æ—Ö—Ä–∞–Ω–∏–º –æ–±–∞ –∞–ª—Ñ–∞–≤–∏—Ç–∞, —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç –≤—Å–µ—Ö –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∫—Ä–æ–º–µ "-". 
# 
# –î–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–æ–≤–æ–≤—Å—Ç–µ–π –∫ –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –º–æ–¥–µ–ª—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º:
# 
# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è;
# 2. –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞ –≤—Å–µ—Ö –±—É–∫–≤ –Ω–∞ –Ω–∏–∂–Ω–∏–π;
# 3. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–æ–π —á–∞—Å—Ç–∏ —Å–ª–æ–≤;
# 4. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (—Ç–∞–∫–∂–µ —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)

# ### Preprocessing textual data

# In[11]:


# Text preprocessing

m = Mystem()
stop_words = set(stopwords.words('russian'))

def preprocessing(sent):
    sent =  re.sub(r'[^\w\s-]', ' ', sent)
    sent = sent.lower()
    
    sent =''.join(m.lemmatize(sent)) 
    
    final_sent = []
    for i in sent.split():
        if i.strip() not in stop_words:
            final_sent.append(i.strip())
    return ' '.join(final_sent)


data['lemm_text'] = data['title'].apply(preprocessing) #function applying to the column


# –í—ã–≤–µ–¥–∏–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —ç–∫—Ä–∞–Ω:

# In[12]:


data.head()


# In[13]:


data['lemm_text'].iloc[57]


# –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã –∏ –ª–µ–º–º–∞—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω—ã. 
# 
# –ü–µ—Ä–µ–π–¥–µ–º –∫ –Ω–µ–±–æ–ª—å—à–æ–º—É –∞–Ω–∞–ª–∏–∑—É.

# ### Analyzing textual data

# –ü–æ—Å—Ç—Ä–æ–∏–º –æ–±–ª–∞–∫–æ —Å–ª–æ–≤ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –Ω–∞–∏–±–æ–ª–µ–µ —É–ø–æ—Ç—Ä–µ–±–∏–º—ã –≤ —Ñ–µ–π–∫–æ–≤—ã—Ö –∏ –Ω–µ —Ñ–µ–π–∫–æ–≤—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö:

# In[14]:


# Separetind dataset into non-fake & fake news titles

fake = data[data['is_fake']==1]
non_fake = data[data['is_fake']==0]


# In[15]:


# Wordcloud function

def disp_wordcloud(data):
    wc = WordCloud(max_words=55, background_color = 'white', 
                   colormap='inferno').generate(' '.join(data['lemm_text'].tolist()))
   
    plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation = 'bilinear')
    plt.axis('off');
    plt.tight_layout(pad=0)


# **`Non-Fake news wordcloud`**

# In[16]:


disp_wordcloud(non_fake) #non-fake news wordcloud


# **`Fake news wordcloud`**

# In[17]:


disp_wordcloud(fake) #fake news wordcloud


# –†–æ—Å—Å–∏—è –Ω–∞ –ø–µ—Ä–≤–æ–º –º–µ—Å—Ç–µ - —ç—Ç–æ –ø–æ–Ω—è—Ç–Ω–æ. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, —Å—É–¥—è –ø–æ "–æ–±–ª–∞–∫–∞–º", —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –±–æ–ª–µ–µ "–ø–µ—Å—Ç—Ä—ã–µ", –æ—Ö–≤–∞—Ç —Ç–µ–º–∞—Ç–∏–∫–∏ —à–∏—Ä–µ: –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–ª–∏—Ç–∏–∫–∞, –Ω–æ –∏ —Å–ø–æ—Ä—Ç, –∏—Å–∫—É—Å—Å—Ç–≤–æ etc. –õ–æ–∂–Ω—ã–µ –∂–µ –±–æ–ª—å—à–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –æ—Å–≤–µ—â–µ–Ω–∏–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.

# <font color='darkblue'><span style="text-decoration:underline">**ANALYZING OUTLIERS**</span></font>

# –ü—Ä–æ–≤–µ–¥–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã—á–∏—Å–ª–∏–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤:

# In[18]:


# Calculating mean word numbers & mean sentence lenght in non-fake and fake news titles

data['word_number'] = data['lemm_text'].apply(lambda x: len(str(x).split()))
data['sentence_length'] = data['lemm_text'].apply(lambda x: len(x) - x.count(' '))

print('–ò—Å—Ç–∏–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏: ')
print('  ‚Ä¢—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ - ', data[data['is_fake']==0]['word_number'].mean().round(2))
print('  ‚Ä¢—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ - ', data[data['is_fake']==0]['sentence_length'].mean().round(2), '\n')

print('–õ–æ–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏: ')
print('  ‚Ä¢—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ - ', data[data['is_fake']==1]['word_number'].mean().round(2))
print('  ‚Ä¢—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ - ', data[data['is_fake']==1]['sentence_length'].mean().round(2)) 


# –ú–∏—Ö–∞–ª –•–æ—Ñ–º–∞–Ω –∫–∞–∫-—Ç–æ —Å–∫–∞–∑–∞–ª: "*–£ –ª–∂–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–æ–≥–∏, –Ω–æ –¥–ª–∏–Ω–Ω—ã–µ —Ä—É–∫–∏*". –ß—Ç–æ –∂–µ, —á–∏—Å—Ç–æ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è, —É –Ω–∞—Å –∏ –Ω–æ–∂–∫–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–¥–ª–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏.
# 
# –§–µ–π–∫–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º –¥–ª–∏–Ω–µ–µ, —á–µ–º –ø—Ä–∞–≤–¥–∏–≤—ã–µ.  

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è c –ø–æ–º–æ—â—å—é –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –∏ "—è—â–∏–∫–∞ —Å —É—Å–∞–º–∏": 

# In[19]:


# Visualizing word distribution of non-fake & fake news titles 

fig, axes = plt.subplots(2, 2, figsize=(14, 8))
fig.suptitle('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µc—Ç–≤–∞ —Å–ª–æ–≤ –≤ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∏ –ª–æ–∂–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö', fontsize=20, color='black')


sns.distplot(data[data['is_fake']==0]['word_number'], bins=10,
             kde=False, color='g', ax=axes[0,0]).set(xlabel='–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤', 
                                                     ylabel='—á–∞—Å—Ç–æ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏', 
                                                     title='–ò—Å—Ç–∏–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏')
sns.boxplot(data[data['is_fake']==0]['word_number'], ax=axes[1,0], color='g').set(xlabel='–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤')

sns.distplot(data[data['is_fake']==1]['word_number'], bins=10,
            kde=False, ax=axes[0,1], color='r').set(xlabel='–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤', title='–õ–æ–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏') 
sns.boxplot(data[data['is_fake']==1]['word_number'], ax=axes[1,1], color='r').set(xlabel='–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤');


# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–∞–ª—å–Ω—ã–π "–∫–æ–ª–æ–∫–æ–ª". –í —Ç–æ –≤—Ä–µ–º—è, –∫–∞–∫ —Ñ–µ–π–∫–æ–≤—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–º–µ—â–µ–Ω—ã –≤–ª–µ–≤–æ –∏–∑-–∑–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è *N–≥–æ* —á–∏—Å–ª–∞ –≤—ã–±—Ä–æ—Å–æ–≤. 
# 
# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:

# In[20]:


# Visualizing sentence length distribution of non-fake & fake news titles 

fig, axes = plt.subplots(2, 2, figsize=(14, 8))
fig.suptitle('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∏ –ª–æ–∂–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö', fontsize=20, color='black')


sns.distplot(data[data['is_fake']==0]['sentence_length'], 
             kde=False, color='g', ax=axes[0,0]).set(xlabel='–¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è', 
                                                     ylabel='—á–∞—Å—Ç–æ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏', 
                                                     title='–ò—Å—Ç–∏–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏')
sns.boxplot(data[data['is_fake']==0]['sentence_length'], ax=axes[1,0], color='g').set(xlabel='–¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è')

sns.distplot(data[data['is_fake']==1]['sentence_length'], 
            kde=False, ax=axes[0,1], color='r').set(xlabel='–¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è', title='–õ–æ–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏') 
sns.boxplot(data[data['is_fake']==1]['sentence_length'], ax=axes[1,1], color='r').set(xlabel='–¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è');


# –í –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±–ª–∏–∑–∫–∏ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º, –Ω–æ —Å–Ω–æ–≤–∞ –ª–æ–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç—Ä–∞–∂–∞—é—Ç –±–æ–ª—å—à–µ–µ —á–∏—Å–ª–æ –≤—ã–±—Ä–æ—Å–æ–≤. 
# 
# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≥—Ä–∞–Ω–∏—Ü—ã —ç—Ç–∏—ä –≤—ã–±—Ä–æ—Å–æ–≤ –∏ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ:

# In[21]:


# function defining the whiskers of a box-plot

def whiskers(data):
    upper_quartile = np.percentile(data, 75)
    lower_quartile = np.percentile(data, 25)

    iqr = upper_quartile - lower_quartile
    lower_whisker = data[data>=lower_quartile-1.5*iqr].min()
    upper_whisker = data[data<=upper_quartile+1.5*iqr].max()
    
    return [lower_whisker, upper_whisker]


# –ü—Ä–∏–º–µ–Ω–∏–º —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏–∏ –∫ –ø–æ–ª—É—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å–ª–æ–≤ –∏ –¥–ª–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:

# In[22]:


# Applying function to columns

columns = ['word_number', 'sentence_length']
for column in columns:
    print(column,  ':', whiskers(data[column]), '\n')


# –ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–æ–µ —á–∏—Å–ª–æ –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å–∞–º–∏:

# In[23]:


# Counting the number of outliers

data_cut = data.query('92 >= sentence_length >= 12 & 11 >= word_number >= 3')
print('–ü–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö —Å–æ—Å—Ç–∞–≤–∏—Ç,', len(data) - len(data_cut), '–Ω–∞–±–ª—é–¥–µ–Ω–∏–π.')


# In[24]:


data_cut['is_fake'].value_counts()


# –í —Ä—è–¥–µ —Å–∏—Ç—É–∞—Ü–∏–π —É–¥–∞–ª–µ–Ω–∏–µ 208 —Å—Ç—Ä–æ–∫ –Ω–µ —Å—Ç–æ–ª—å –∑–Ω–∞—á–∏–º–æ. –ù–û! –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –º—ã –∏—Å–∫–ª—é—á–∏–º 205 —Å—Ç—Ä–æ–∫ —Å —Ñ–µ–π–∫–æ–≤—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —á–∞—Å—Ç–∏—á–Ω–æ –ª–∏—à–∏–º –º–æ–¥–µ–ª–∏ –≤–∞–∂–Ω–æ–π "–ø–∏—â–∏" –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. 
# 
# –ö–∞–∫ –≤–∏–¥–∏–º, –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –º–µ—Ä–µ, —ç—Ç–æ —Ñ–∏—à–∫–∞ —Ñ–µ–π–∫–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –∑–Ω–∞—á–∏–º–æ–µ —Å–≤–æ–π—Å—Ç–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ –¥–æ–ª–∂–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —Å–∫–æ—Ä–∞.

# –¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –ú–æ–∂–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ —ç—Ç–∞–ø—É –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –æ–±—É—á–µ–Ω–∏—é.

# ## Preparation for ML

# –°–Ω–∞—á–∞–ª–∞ —Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º Baseline Model –Ω–∞ –æ—Å–Ω–æ–≤–Ω–µ –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TfidfVectorizer().
# 
# * –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ 
# * —Ä–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç—Ä–µ–π–Ω –∏ –≤–∞–ª–∏–¥
# * –ø—Ä–æ–≤–µ–¥–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é
# * –æ–±—É—á–∏–º –º–æ–¥–µ–ª—å –∏ –≤—ã–≤–µ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–µ—Ç—Ä–∏–∫–∏ **F1**.

# ### Baseline Model

# In[25]:


# Baseline mmodel variables

X_baseline = data['title']  
y_baseline = data['is_fake']


# In[26]:


X_train_baseline, X_valid_baseline, y_train, y_valid = train_test_split(X_baseline, y_baseline, 
                                                                        test_size=0.3, random_state=0)


# In[27]:


tf_idf = TfidfVectorizer() 

X_train_baseline_vect = tf_idf.fit_transform(X_train_baseline)
X_valid_baseline_vect = tf_idf.transform(X_valid_baseline)

print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: ', X_train_baseline_vect.shape)
print('–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: ', X_valid_baseline_vect.shape)


# In[28]:


baseline_model = LogisticRegression().fit(X_train_baseline_vect, y_train)

baseline_preds = baseline_model.predict(X_valid_baseline_vect)
baseline_f1 = f1_score(y_valid, baseline_preds)

print('F1_score: ', baseline_f1.round(2))


# –î–∞–∂–µ –Ω–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ —Ç–∞–∫–æ–π —É–∂ –∏ –Ω–∏–∑–∫–∏–π. –ü–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –ø–æ–∫–∞–∂–µ—Ç "–ø–æ–¥–∫—Ä—É—Ç–∫–∞" –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ "—á–∏—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ".

# ### Main models

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ç–∞—Ä–≥–µ—Ç–∞. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –ø—Ä–∏–∑–Ω–∞–∫ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω - —ç—Ç–æ –æ—á–∏—â–µ–Ω–Ω—ã–µ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. 

# In[29]:


# Main variables for ML

X = data['lemm_text']  
y = data['is_fake']


# –†–∞–∑–¥–µ–ª–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–∞ –¥–≤–µ –≤—ã–±–æ—Ä–∫–∏:

# In[30]:


# Splitting data

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)


# –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **TfidfVectorizer**, –∞ –Ω–µ **CountVectorizer**, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å–æ —Å–º–µ—â–µ–Ω–∏–µ–º –≤ –ø–æ–ª—å–∑—É –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ –∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—é —Ä–µ–¥–∫–∏—Ö. TfidfVectorizer –ø–æ–º–æ–∂–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —ç—Ç–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.

# –ò—Å–ø–æ–ª—å–∑—É—è TfidfVectorizer, –ø—Ä–∏–º–µ–Ω–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã *`ngram_range=(1, 2), use_idf=False`*, –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.
# 
# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–∏–º —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.

# In[31]:


# Vectorizing the feature

tf_idf = TfidfVectorizer(ngram_range=(1, 2), use_idf=False) 

X_train_vect = tf_idf.fit_transform(X_train)
X_valid_vect = tf_idf.transform(X_valid)

print('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: ', X_train_vect.shape)
print('–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: ', X_valid_vect.shape)


# –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã–µ, –º–æ–∂–Ω–æ –ø—Ä–∏—Å—Ç—É–ø–∞—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é.

# **–î–ª—è –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–æ –≤—ã–±—Ä–∞–Ω–æ 5 –º–æ–¥–µ–ª–µ–π:**
# 
# 1. `–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è;`
# 2. `–ü–∞—Å—Å–∏–≤–Ω–æ-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä;`
# 3. `–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–π –Ω–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä;`
# 4. `–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤;`
# 5. `–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω.`

# –î–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏, —á–µ–º –¥–µ—Ä–µ–≤—è–Ω–Ω—ã–µ –∏–ª–∏ –±—É—Å—Ç–∏–Ω–≥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏.
# 
# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±—É–¥–µ—Ç –ø—Ä–æ—Å–∏—Ö–æ–¥–∏—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏. –ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≤—ã—á–∏—Å–ª–∏–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É **F1**, –∞ —Ç–∞–∫–∂–µ –ø–æ–ª–Ω–æ—Ç—É –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–æ–≥–æ–Ω–æ–∑–æ–≤. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø–æ—Å—Ç—Ä–æ–∏–≤ –º–∞—Ç—Ä–∏—Ü—É –ø—É—Ç–∞–Ω–∏—Ü—ã.

#  **`LogisticRegression`**

# In[32]:


#LogisticRegression

model_lr = LogisticRegression(random_state=0)

params_lr = {'solver': ['newton-cg', 'lbfgs', 'liblinear'],
             'C': [5, 15, 25]
            }
grid_lr = GridSearchCV(model_lr, params_lr, 
                       scoring='f1', n_jobs=-1, cv=5, refit=True)
grid_lr.fit(X_train_vect, y_train)

preds_lr = grid_lr.predict(X_valid_vect)
recall_lr = recall_score(y_valid, preds_lr)
precision_lr = precision_score(y_valid, preds_lr)
f1_lr = f1_score(y_valid, preds_lr)

print('–ü–æ–ª–Ω–æ—Ç–∞: ', recall_lr)
print('–¢–æ—á–Ω–æ—Å—Ç—å: ', precision_lr)
print('F1_score: ', f1_lr, '\n')

best_params_lr = grid_lr.best_params_
print('–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: ', best_params_lr)


# In[33]:


# Confusion matrix - LogisticRegression
sns.heatmap(confusion_matrix(y_valid, preds_lr), fmt='3.0f', annot=True, cmap='RdYlGn', annot_kws={'size':17})
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: LogisticRegression', size=15);


# –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—ã—à–µ –±–µ–π–∑–ª–∞–π–Ω–∞, –Ω–æ –≤—Å–µ-—Ç–∞–∫–∏ –Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –º–æ–¥–µ–ª—å –Ω–µ–ø–ª–æ—Ö–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–æ–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, —Ö–æ—Ç—è –∏ –ø—Ä–∏–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ç–∏–Ω–Ω—ã–º –Ω–æ–≤–æ—Å—Ç—è–º —Å—Ç–∞—Ç—É—Ç "–ª–æ–∂–Ω–æ—Å—Ç–∏", —Ç–µ—Ä—è—è, —Ç–µ–º —Å–∞–º—ã–º, –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏.  

# **`Passive Aggressive Classifier`**

# In[34]:


# PassiveAggressiveClassifier

model_pac = PassiveAggressiveClassifier(shuffle=False, random_state=0)

params_pac = {'C': [10, 15, 25]}

grid_pac = GridSearchCV(model_pac, params_pac, 
                        scoring='f1', n_jobs=-1, cv=5, refit=True)
grid_pac.fit(X_train_vect, y_train)

preds_pac = grid_pac.predict(X_valid_vect)
recall_pac = recall_score(y_valid, preds_pac)
precision_pac = precision_score(y_valid, preds_pac)
f1_pac = f1_score(y_valid, preds_pac)


print('–ü–æ–ª–Ω–æ—Ç–∞: ', recall_pac)
print('–¢–æ—á–Ω–æ—Å—Ç—å: ', precision_pac)
print('F1_score: ', f1_pac, '\n')

best_params_pac = grid_pac.best_params_
print('–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: ', best_params_pac)


# In[35]:


# Confusion matrix - PassiveAggressiveClassifier

sns.heatmap(confusion_matrix(y_valid, preds_pac), fmt='3.0f', annot=True, cmap='RdYlGn', annot_kws={'size':17})
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: PassiveAggressiveClassifier', size=15);


# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–∞–∫–æ–π –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∫–∞–∫ –∏ —Å –õ–æ–≥–∏—Å—Ç–∏—á–µ–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π, —Ç–æ–ª—å–∫–æ —á—É—Ç—å –±–æ–ª—å—à–µ –æ—à–∏–±–æ–∫ –≤—Ç–æ—Ä–æ–≥–æ —Ä–æ–¥–∞.

# **`MultinomialNB`**

# In[36]:


# MultinomialNB

model_mnb = MultinomialNB()

params_mnb = {'alpha': [1e-1, 1e-3, 1e-6]
             }

grid_mnb = GridSearchCV(model_mnb, params_mnb,
                        scoring='f1', cv=5, refit=True)
grid_mnb.fit(X_train_vect, y_train)

preds_mnb = grid_mnb.predict(X_valid_vect)
recall_mnb = recall_score(y_valid, preds_mnb)
precision_mnb = precision_score(y_valid, preds_mnb)
f1_mnb = f1_score(y_valid, preds_mnb)

print('–ü–æ–ª–Ω–æ—Ç–∞: ', recall_mnb)
print('–¢–æ—á–Ω–æ—Å—Ç—å: ', precision_mnb)
print('F1_score: ', f1_mnb, '\n')

best_params_mnb = grid_mnb.best_params_
print('–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: ', best_params_mnb)


# In[37]:


# Confusion matrix - MultinomialNB

sns.heatmap(confusion_matrix(y_valid, preds_mnb), fmt='3.0f', annot=True, cmap='RdYlGn', annot_kws={'size':17})
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: MultinomialNB', size=15);


# –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ª–æ–∂–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∞ –≤–µ—Ä–Ω–æ. –ü—Ä–∏ —ç—Ç–æ–º –Ω–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏.  

# **`Support Vector Machine`**

# In[38]:


# Support Vector Machine

model_svc = SVC() 
param_svc = {'C': [5, 10, 15], 
              'gamma': [1,0.1,0.01,0.001],
              'kernel': ['linear','sigmoid']
             }
grid_svc = GridSearchCV(model_svc, param_svc, scoring='f1', refit=True, cv=5, verbose=0)
grid_svc.fit(X_train_vect, y_train)


preds_svc = grid_svc.predict(X_valid_vect)
recall_svc = recall_score(y_valid, preds_svc)
precision_svc = precision_score(y_valid, preds_svc)
f1_svc = f1_score(y_valid, preds_svc)

print('–ü–æ–ª–Ω–æ—Ç–∞: ', recall_svc)
print('–¢–æ—á–Ω–æ—Å—Ç—å: ', precision_svc)
print('F1_score: ', f1_svc)

best_params_svc = grid_svc.best_params_
print('–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: ', best_params_svc)


# In[39]:


# Confusion matrix - Support Vector Machine

sns.heatmap(confusion_matrix(y_valid, preds_svc), fmt='3.0f', annot=True, cmap='RdYlGn', annot_kws={'size':17})
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: Support Vector Machine', size=15);


# –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ö–æ–∂ —Å –ø–µ—Ä–≤—ã–º–∏ –¥–≤—É–º—è –º–æ–¥–µ–ª—è–º–∏ - –Ω–µ —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –º–µ—Ç—Ä–∏–∫–∏ **F1**.

# **`MLPClassifier`**

# –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å "*one eternity later*", —Å–¥–µ–ª–∞–µ–º –¥–ª—è –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–±–µ–≥–Ω–µ–º –∫ —Ä—É—á–Ω–æ–º—É –ø–æ–¥–±–æ—Ä—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ üßÆ.

# In[40]:


# MLPClassifier

model_mlp = MLPClassifier(hidden_layer_sizes=(7, 10, 7), batch_size=295, shuffle=False, random_state=0)

model_mlp.fit(X_train_vect, y_train)

preds_mlp = model_mlp.predict(X_valid_vect)
recall_mlp = recall_score(y_valid, preds_mlp)
precision_mlp = precision_score(y_valid, preds_mlp)
f1_mlp = f1_score(y_valid, preds_mlp)


print('–ü–æ–ª–Ω–æ—Ç–∞: ', recall_mlp)
print('–¢–æ—á–Ω–æ—Å—Ç—å: ', precision_mlp)
print('F1_score: ', f1_mlp)


# In[41]:


# Confusion matrix - MLPClassifier

sns.heatmap(confusion_matrix(y_valid, preds_mlp), fmt='3.0f', annot=True, cmap='RdYlGn', annot_kws={'size':17})
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫: MLPClassifier', size=15);


# –°–æ–≤—Å–µ–º –Ω–µ–º–Ω–æ–≥–æ (–≤—Å–µ–≥–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –µ–¥–∏–Ω–∏—Ü), –Ω–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ–∫–∞–∑–∞–ª–∞—Å—å —á—É—Ç—å –ª—É—á—à–µ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞. 

# –ü–æ—Å—Ç—Ä–æ–∏–º –¥–ª—è MLPClassifier AUC-ROC –∫—Ä–∏–≤—É—é: 

# In[42]:


# AUC-ROC curve

probabilities_valid = model_mlp.predict_proba(X_valid_vect)
probabilities_one_valid = probabilities_valid[:, 1]
auc_roc_valid = roc_auc_score(y_valid, probabilities_one_valid)
print('AUC-ROC:', auc_roc_valid)
print('')

fpr, tpr, thresholds = roc_curve(y_valid, probabilities_one_valid)

plt.figure(facecolor='silver')
plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1])
plt.grid(True)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.ylim([0.0, 1.0])
plt.xlim([0.0, 1.0])
plt.title('ROC-–∫—Ä–∏–≤–∞—è');


# –ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ AUC-ROC –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–æ–µ. –ö—Ä–∏–≤–∞—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–ª—å–Ω–æ –ø–æ–¥–Ω–∏–º–∞–µ—Ç—Å—è –Ω–∞–¥ –ª–∏–Ω–∏–µ–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.  
# 
# –ù–∞—à–∞ –º–æ–¥–µ–ª—å –Ω–µ–ø–ª–æ—Ö–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å "1": —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç, –∫–∞–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —è–≤–ª—è—é—Ç—Å—è –ª–æ–∂–Ω—ã–º–∏. –ù–µ—Å–∫–æ–ª—å–∫–æ —Ö—É–∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–ª–∞—Å—Å–æ–º "0": –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–≤–∞ —Ä–∞–∑–∞ –±–æ–ª—å—à–µ, —á–µ–º –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –ò –≤—Å–µ –∂–µ, –µ—Å–ª–∏ –º—ã –ø–æ—Å—Ç–∞—Ä–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å, —Ç–æ –Ω–∞—á–Ω–µ–º —Ç–µ—Ä—è—Ç—å –≤ –ø–æ–ª–Ω–æ—Ç–µ - —Ç–∞–∫–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, —Ç–∞–∫ –∫–∞–∫ –≤–∞–∂–Ω–µ–µ –≤—ã—á–ª–µ–Ω–∏—Ç—å —Ñ–µ–π–∫–∏.

# ### Result

# –°–≤–µ–¥–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª–Ω–æ f1-score:

# In[43]:


# Resulting table

score_table = pd.DataFrame(data=[[f1_lr, recall_lr, precision_lr], 
                                [f1_pac, recall_pac, precision_pac],
                                [f1_svc, recall_svc, precision_svc],
                                [f1_mnb, recall_mnb, precision_mnb], 
                                [f1_mlp, recall_mlp, precision_mlp]],
                      columns = ['f1_score', 'Recall', 'Precision'], 
                      index = ['LogReg', 'PasAgg', 'SVC', 'MNB',  'MLP']).sort_values(by='f1_score', ascending=False)
score_table


# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ **F1** –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:

# In[44]:


# F1-score visualisation 

def scores(data):
    fig, ax = plt.subplots(figsize=(8,4))
   
    ax.vlines(x=data.index, ymin=0, ymax=data['f1_score'], color='dimgrey', alpha=0.7, linewidth=2)
    ax.scatter(x=data.index, y=data['f1_score'], s=75, color='firebrick', alpha=0.85)
   
    ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1-–º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ', size=16, y=(1.02))
    ax.set_ylabel('F1_score')
    ax.set_ylim(0, 1.5)
    ax.set_xticks(data.index)
    ax.set_xticklabels(data.index, rotation=0, fontdict={'horizontalalignment': 'center', 'size':12})

    for row in data.itertuples():
        ax.text(row.Index, row.f1_score +.2, s=round(row.f1_score, 2), 
           horizontalalignment= 'center', verticalalignment='bottom', fontsize=14);
        
scores(score_table)


# –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏–¥—É—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤—Ä–æ–≤–µ–Ω—å, –Ω–æ MLPClassifier() —á—É—Ç—å –ª—É—á—à–µ. –ù–∞ –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –æ—Å—Ç–∞–Ω–æ–≤–∏–º —Å–≤–æ–π –≤—ã–±–æ—Ä, —á—Ç–æ–±—ã —Å–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∂–Ω–æ—Å—Ç—å & –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. 
# 
# –ò—Ç–∞–∫, –ø–µ—Ä–µ–π–¥–µ–º –∫ –∏—Ç–æ–≥–æ–≤–æ–π —á–∞—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞ - —Ä–∞–±–æ—Ç–µ –º–æ–¥–µ–ª–∏ –≤ –ø–æ–ª–µ–≤—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. 

# ## Predictions & results

# –§–∏–Ω–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞. 
# 
# –î–ª—è –ø—Ä–µ–¥–∞—Å–∫–∞–∑–∞–Ω–∏–π –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞ —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

# ### Importing and reading test data

# –ó–∞–≥—Ä—É–∑–∏–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ

# In[45]:


# Reading test dataset

predictions = pd.read_csv('test.tsv', sep='\t')


# In[46]:


predictions.head()


# In[47]:


predictions.info()


# –î–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ - –≤—Å–µ–≥–æ 1000 —Å—Ç—Ä–æ–∫.

# ### Preprocessing 

# –û–±—Ä–∞–±–æ—Ç–∞–µ–º —Ç–µ–∫—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –∏–º–µ—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞.

# In[48]:


# Preprocessing textual data

predictions['lemm_text'] = predictions['title'].apply(preprocessing) 


# ### Vectorizing

# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –∫ –æ–±—É—á–µ–Ω–∏—é.

# In[49]:


# Vectorizing the feature

X_test = predictions['lemm_text']
X_test = tf_idf.transform(X_test)


# ### Predicting

# –ò—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å MLPClassifier() —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å–¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:

# In[50]:


# Making predictions

predictions['is_fake'] = model_mlp.predict(X_test)


# ### Results 

# –£–¥–∞–ª–∏–º –Ω–µ–Ω—É–∂–Ω—ã–π –±–æ–ª–µ–µ —Å—Ç–æ–ª–±–µ—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:

# In[51]:


del predictions['lemm_text']


# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

# In[52]:


predictions.head()


# In[53]:


# Fake amd non-fake news title ratio - test data

ratio = predictions['is_fake'].value_counts()
print(ratio, '\n');

plt.figure(figsize=(5,5))
plt.title('–î–æ–ª—è –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∏ –ª–æ–∂–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π', fontsize=14, color='black')

labels=['NON-FAKE', 'FAKE']
ratio.plot(kind='pie', startangle=90, textprops={'fontsize': 14},
           labels=labels, autopct = '%0.0f%%', colors = ['blue', 'yellow'])
plt.ylabel('');


# ### Exporting final file

# In[54]:


# Exporting file

predictions.to_csv('predictions.tsv', sep='\t', index=False)


# ## Conclusion

# –ú—ã –∑–∞–≥—Ä—É–∑–∏–ª–∏ –æ–±–∞ –∏–º—é—â–∏—Ö—Å—è —Ñ–∞–π–ª–∞. –ù–∞–ø–∏—Å–∞–ª–∏ —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –µ–µ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–æ–∫. 
# 
# –í–µ–∫—Ç–æ—Ä–µ–∑–∏—Ä–æ–≤–∞–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é TfidfVectorizer(). –ü—Ä–æ–≤–µ—Ä–∏–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ª—É—á—à–µ–π –æ–∫–∑–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å MLPClassifier. –£–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–∏—á—å –ø–æ–∫–∞–∑–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ F1, —Ä–∞–≤–Ω–æ–µ 0.87. –¢–∞–∫–∂–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ç–æ—á–Ω–µ–µ –≤—ã—è–≤–ª—è—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ª–æ–∂–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–µ–π–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. 
# 
# –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ –≤—ã–¥—É–º–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç –±—ã–ª —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª. 

# <img src="https://miro.medium.com/focal/116/116/50/50/1*9wEfSMQ93nMWyBq8HdtVow.jpeg" width="200">
